{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZI2bHmId4aMM"
   },
   "source": [
    "# Video Intelligence API\n",
    "\n",
    "In this notebook, we'll use the Video Intelligence API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cKLmgBFT4Np"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 794
    },
    "executionInfo": {
     "elapsed": 19144,
     "status": "ok",
     "timestamp": 1704373544715,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -540
    },
    "id": "GEzSnfTuRnjg",
    "outputId": "00491735-aeca-4733-bbdb-ea0eabd984c8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install ipython google-cloud-videointelligence\n",
    "!pip3 install pandas \n",
    "!pip3 install Pillow\n",
    "!pip3 install opencv-python\n",
    "!pip3 install google-cloud-vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSEyKO4BR6ho"
   },
   "source": [
    "You might have to restart your runtime to load these packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 485,
     "status": "ok",
     "timestamp": 1704379810580,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -540
    },
    "id": "bEbqffN68meY",
    "outputId": "be5e16b2-eeb4-4b57-f77e-7b8cb1f72885",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 506,
     "status": "ok",
     "timestamp": 1704379833838,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -540
    },
    "id": "7qUC6u4J8vu3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"your-project-id\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Enable video intelligence API (Execute one time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1805,
     "status": "ok",
     "timestamp": 1704379851794,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -540
    },
    "id": "abLT4_mktV2V",
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gcloud services enable videointelligence.googleapis.com --project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 488,
     "status": "ok",
     "timestamp": 1704379857917,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -540
    },
    "id": "hQUBRTxSb23W",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw\n",
    "from google.cloud import videointelligence_v1 as vi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pM8lpMk6hPdu"
   },
   "source": [
    "### Authentication (Colab only)\n",
    "\n",
    "If you are running this notebook on Colab, you will need to run the following cell authentication. This step is not required if you are using Vertex AI Workbench as it is pre-authenticated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 494,
     "status": "ok",
     "timestamp": 1704379865991,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -540
    },
    "id": "W5xKz_yobDmX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# if it's Colab runtime, authenticate the user with Google Cloud\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 408,
     "status": "ok",
     "timestamp": 1704375530529,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -540
    },
    "id": "GywERSiHbMXJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET = \"bucket-name\"\n",
    "video_file = \"video1.mp4\"\n",
    "\n",
    "video_gcs_uri = f\"gs://{BUCKET}/{video_file}\"     \n",
    "video_path = f\"./video/{video_file}\"                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logo Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from typing import Optional, Sequence, cast\n",
    "\n",
    "from google.cloud import videointelligence_v1 as vi\n",
    "\n",
    "\n",
    "def detect_logos(\n",
    "    video_uri: str, segments: Optional[Sequence[vi.VideoSegment]] = None\n",
    ") -> vi.VideoAnnotationResults:\n",
    "    video_client = vi.VideoIntelligenceServiceClient()\n",
    "    features = [vi.Feature.LOGO_RECOGNITION]\n",
    "    context = vi.VideoContext(segments=segments)\n",
    "    request = vi.AnnotateVideoRequest(\n",
    "        input_uri=video_uri,\n",
    "        features=features,\n",
    "        video_context=context,\n",
    "    )\n",
    "\n",
    "    print(f'Processing video \"{video_uri}\"...')\n",
    "    operation = video_client.annotate_video(request)\n",
    "\n",
    "    # Wait for operation to complete\n",
    "    response = cast(vi.AnnotateVideoResponse, operation.result())\n",
    "    # A single video is processed\n",
    "    results = response.annotation_results[0]\n",
    "\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_detected_logos(results: vi.VideoAnnotationResults):\n",
    "    annotations = results.logo_recognition_annotations\n",
    "    #print(annotations)\n",
    "\n",
    "    print(f\" Detected logos: {len(annotations)} \".center(80, \"-\"))\n",
    "    for annotation in annotations:\n",
    "        entity = annotation.entity\n",
    "        entity_id = entity.entity_id\n",
    "        description = entity.description\n",
    "        for track in annotation.tracks:\n",
    "            confidence = track.confidence\n",
    "            t1 = track.segment.start_time_offset.total_seconds()\n",
    "            t2 = track.segment.end_time_offset.total_seconds()\n",
    "            logo_frames = len(track.timestamped_objects)\n",
    "            print(\n",
    "                f\"{confidence:4.0%}\",\n",
    "                f\"{t1:>7.3f}\",\n",
    "                f\"{t2:>7.3f}\",\n",
    "                f\"{logo_frames:>3} fr.\",\n",
    "                f\"{entity_id:<15}\",\n",
    "                f\"{description}\",\n",
    "                sep=\" | \",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import vision\n",
    "def detect_logo_text(img_path):\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "    \n",
    "    with open(img_path, \"rb\") as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = vision.Image(content=content)\n",
    "    \n",
    "    \n",
    "    #im = Image.open(\"./1_frame.jpg\")    \n",
    "\n",
    "    response = client.text_detection(image=image)\n",
    "    #print(response.full_text_annotation.text)\n",
    "    \n",
    "    return response.full_text_annotation.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def extract_logo_frames(index:int, video_name, start_time:float, end_time:float, box):\n",
    "    print(f\" extract frame: {index} \".center(80, \"-\"))\n",
    "\n",
    "    cap = cv2.VideoCapture(f\"./video/{video_name}\")\n",
    "    #print(f\" start time ~ end time :{start_time} ~ {end_time}\" )\n",
    "\n",
    "    # Check if the video was opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        exit()\n",
    "\n",
    "    # Get the frame rate of the video\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Calculate the total number of frames to extract\n",
    "    total_frames = int((end_time - start_time) * fps)\n",
    "    #print(f\"total+frames  :{total_frames}\")\n",
    "\n",
    "    # Set the starting frame position\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_time * fps)\n",
    "\n",
    "    # Initialize a frame counter\n",
    "    frame_count = 0\n",
    "    image_name = \"\"\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    directory_path = f\"./logos/{video_name}\"\n",
    "    if ret:\n",
    "        if not os.path.exists(directory_path):\n",
    "            os.makedirs(directory_path)\n",
    "        image_name = f\"{directory_path}/{index}_frame.jpg\"\n",
    "        cv2.imwrite(image_name, frame)\n",
    "        #print(f\"image name : {image_name}\")\n",
    "                      \n",
    "    width = 1280\n",
    "    height = 720\n",
    "    if image_name : \n",
    "        im = Image.open(image_name)  \n",
    "        left = box.left * width\n",
    "        bottom = box.bottom * height\n",
    "        right = box.right * width\n",
    "        top = box.top * height\n",
    "        #print(f\" {left} {bottom} {right} {top}\" )\n",
    "        #        im2 = im.crop([vects[0].x, vects[0].y , vects[2].x , vects[2].y ])\n",
    "        im_crop = im.crop([left, top, right, bottom ])\n",
    "        width, height = im_crop.size\n",
    "        \n",
    "        im_crop.save(f\"{directory_path}/logo_{index}.jpg\", \"JPEG\")    \n",
    "        detected_text=detect_logo_text(f\"{directory_path}/logo_{index}.jpg\")\n",
    "        print(\n",
    "            f\"Logo Size : {im_crop.size}\",\n",
    "            f\"Logo w:h Ratio : {width/height:.1f} : 1\",\n",
    "            f\"Logo Location (top,left) : ({top:.1f}, {left:.1f})\",\n",
    "            f\"Text : {detected_text}\",\n",
    "            sep=\" | \",\n",
    "        )\n",
    "\n",
    "        display(im_crop)\n",
    "        \n",
    "        \n",
    "    # Release the VideoCapture object\n",
    "    cap.release()\n",
    "    \n",
    "    #extract_logo_by_vision(image_name)\n",
    "    \n",
    "    return image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "def print_image_frames(images):\n",
    "    #print(images)\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 12))  # Adjust figure size as needed\n",
    "    row =len(images) // 4 + 1\n",
    "\n",
    "    gs = gridspec.GridSpec(row, 4, figure=fig)  # 2 rows, 3 columns\n",
    "\n",
    "    # Iterate through the images and plot each in a subplot\n",
    "    for i, image_path in enumerate(images):\n",
    "        ax = fig.add_subplot(gs[i])  # Place image in the grid\n",
    "        ax.imshow(plt.imread(image_path))\n",
    "        ax.axis('off')  # Turn off axes for cleaner display\n",
    "\n",
    "    # Customize spacing and layout\n",
    "    plt.tight_layout()  # Adjust spacing between subplots\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_logo_frames(results: vi.VideoAnnotationResults, entity_id: str):\n",
    "    def keep_annotation(annotation: vi.LogoRecognitionAnnotation) -> bool:\n",
    "        return annotation.entity.entity_id == entity_id\n",
    "\n",
    "    annotations = results.logo_recognition_annotations\n",
    "    annotations = [a for a in annotations if keep_annotation(a)]\n",
    "    for annotation in annotations:\n",
    "        description = annotation.entity.description\n",
    "        for track in annotation.tracks:\n",
    "            confidence = track.confidence\n",
    "            print(\n",
    "                f\" {description},\"\n",
    "                f\" confidence: {confidence:.0%},\"\n",
    "                f\" frames: {len(track.timestamped_objects)} \".center(80, \"-\")\n",
    "            )\n",
    "            for timestamped_object in track.timestamped_objects:\n",
    "                t = timestamped_object.time_offset.total_seconds()\n",
    "                box = timestamped_object.normalized_bounding_box\n",
    "                print(box)\n",
    "                print(\n",
    "                    f\"{t:>7.3f}\",\n",
    "                    f\"({box.left:.5f}, {box.top:.5f})\",\n",
    "                    f\"({box.right:.5f}, {box.bottom:.5f})\",\n",
    "                    sep=\" | \",\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_detected_logos_extract_frames(results: vi.VideoAnnotationResults, video_name):\n",
    "    annotations = results.logo_recognition_annotations\n",
    "    #print(annotations)\n",
    "    \n",
    "    index=0;\n",
    "    images = [] \n",
    "    logos = []\n",
    "\n",
    "    print(f\" Detected logos: {len(annotations)} \".center(80, \"-\"))\n",
    "    for annotation in annotations:\n",
    "        entity = annotation.entity\n",
    "        entity_id = entity.entity_id\n",
    "        description = entity.description\n",
    "        for track in annotation.tracks:\n",
    "            confidence = track.confidence\n",
    "            t1 = track.segment.start_time_offset.total_seconds()\n",
    "            t2 = track.segment.end_time_offset.total_seconds()\n",
    "            objects = track.timestamped_objects\n",
    "            logo_frames = len(objects)\n",
    "\n",
    "            if entity_id == \"/m/03068d\" : # \"LG Electronics\"\n",
    "               \n",
    "                if logo_frames >= 2 :\n",
    "                    image = extract_logo_frames(index, video_name, t1, t1+ objects[1].time_offset.total_seconds(), objects[1].normalized_bounding_box)\n",
    "                    if image:\n",
    "                        images.append(image)\n",
    "\n",
    "                else:\n",
    "                    image = extract_logo_frames(index, t1, t2, objects[0].normalized_bounding_box)\n",
    "                    if image:\n",
    "                        images.append(image)\n",
    "             \n",
    "                #images.append(extract_logo_frames(index, t1,t2))\n",
    "                index += 1\n",
    "            \n",
    "            print(\n",
    "                f\"{confidence:4.0%}\",\n",
    "                f\"{t1:>7.3f}\",\n",
    "                f\"{t2:>7.3f}\",\n",
    "                f\"{t2-t1:>7.3f} secs\",\n",
    "                f\"{logo_frames:>3} fr.\",\n",
    "                f\"{entity_id:<15}\",\n",
    "                f\"{description}\",\n",
    "                sep=\" | \",\n",
    "            )\n",
    "    #print(images)        \n",
    "    print_image_frames(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_uri = video_gcs_uri\n",
    "'''\n",
    "segment = vi.VideoSegment(\n",
    "    start_time_offset=timedelta(seconds=146),\n",
    "    end_time_offset=timedelta(seconds=156),\n",
    ")\n",
    "results = detect_logos(video_uri, [segment])\n",
    "'''\n",
    "results = detect_logos(video_uri )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print_detected_logos(results)\n",
    "#print_logo_frames(results)\n",
    "video_name = video_file\n",
    "print(video_file)\n",
    "print_detected_logos_extract_frames(results, video_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect Shot Change\n",
    "Shot Change와 Object Tracking을 통해서 움직임이 없는 동영상 검수\n",
    "- Shot Change : 동영상이 장면이 바뀌는 듯한 변화가 감지되는것 \n",
    "- shot이 1개 인 동영상에 대해서만 Object Tracking을 추가로 검수\n",
    "- Tracking된 Object가 shot의 start에서 부터 end까지 보여지는 지 확인\n",
    "- start에서 end까지 보여진다면 위치 변화가 없는지 확인 : 어느 정도의 px까지의 움직임을 확인할 것인지 정할 필요 있음, 움직임이 없다고 보내준 동영상이 실제로는 object의 px위치에서 6px 정도의 움직임이 있었(아래 예시는 10px 이하로 허용하는 것으로 구현)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from typing import Optional, Sequence, cast\n",
    "\n",
    "from google.cloud import videointelligence_v1 as vi\n",
    "\n",
    "\n",
    "def detect_shot_change(\n",
    "    video_uri: str, segments: Optional[Sequence[vi.VideoSegment]] = None\n",
    ") -> vi.VideoAnnotationResults:\n",
    "    video_client = vi.VideoIntelligenceServiceClient()\n",
    "    features = [vi.Feature.SHOT_CHANGE_DETECTION, vi.Feature.OBJECT_TRACKING,] \n",
    "    context = vi.VideoContext(segments=segments)\n",
    "    request = vi.AnnotateVideoRequest(\n",
    "        input_uri=video_uri,\n",
    "        features=features,\n",
    "        video_context=context,\n",
    "    )\n",
    "\n",
    "    print(f'Processing video \"{video_uri}\"...')\n",
    "    operation = video_client.annotate_video(request)\n",
    "\n",
    "    # Wait for operation to complete\n",
    "    response = cast(vi.AnnotateVideoResponse, operation.result())\n",
    "    # A single video is processed\n",
    "    results = response.annotation_results[0]\n",
    "    #print(response)\n",
    "    #print(\"-----\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def extract_shot_frames(index:int, video_name, start_time:float, end_time:float):\n",
    "    print(f\" extract frame: {index} \".center(80, \"-\"))\n",
    "\n",
    "    cap = cv2.VideoCapture(f\"./video/{video_name}\")\n",
    "    #print(f\" start time ~ end time :{start_time} ~ {end_time}\" )\n",
    "\n",
    "    # Check if the video was opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        exit()\n",
    "\n",
    "    # Get the frame rate of the video\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Calculate the total number of frames to extract\n",
    "    total_frames = int((end_time - start_time) * fps)\n",
    "    #print(f\"total+frames  :{total_frames}\")\n",
    "    middle_frame = total_frames // 2\n",
    "\n",
    "    # Set the starting frame position\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_time * fps + middle_frame)\n",
    "\n",
    "    # Initialize a frame counter\n",
    "    frame_count = 0\n",
    "    image_name = \"\"\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        directory_path = f\"./shots/{video_name}\"\n",
    "        if not os.path.exists(directory_path):\n",
    "            os.makedirs(directory_path)\n",
    "        image_name = f\"{directory_path}/{index}_frame.jpg\"\n",
    "        cv2.imwrite(image_name, frame)\n",
    "        print(f\"image name : {image_name}\")\n",
    "                             \n",
    "    # Release the VideoCapture object\n",
    "    cap.release()\n",
    "    \n",
    "    #extract_logo_by_vision(image_name)\n",
    "    \n",
    "    return image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_detected_shot_change(results: vi.VideoAnnotationResults):\n",
    "    #print(results)\n",
    "    shot_annotations = results.shot_annotations\n",
    "    #print(annotations)\n",
    "    \n",
    "    motion_detected = True \n",
    "    \n",
    "    width = 1280\n",
    "    height = 720\n",
    "    \n",
    "    shot_len = len(shot_annotations)\n",
    "    print(f\" Shot Changess: {shot_len} \".center(80, \"-\"))\n",
    "    \n",
    "    \n",
    "    if shot_len == 1 : \n",
    "        t1 = shot_annotations[0].start_time_offset.total_seconds()\n",
    "        t2 = shot_annotations[0].end_time_offset.total_seconds()\n",
    "        shot_secs = t2-t1\n",
    "\n",
    "        print(f\"{t1:>7.3f} | {t2:>7.3f} | {shot_secs:>7.3f} secs\")\n",
    "        \n",
    "        a_index = 0\n",
    "        annotations = results.object_annotations\n",
    "        for annotation in annotations:\n",
    "\n",
    "            object_sec = annotation.frames[-1].time_offset.total_seconds()\n",
    "            \n",
    "            #print(f\" a index {a_index}\")\n",
    "            if abs(shot_secs - object_sec) < 0.1 :\n",
    "                print(f\" object is shown throuhput the shot : Diff : {abs(shot_secs - object_sec):.3f} secs\".center(80, \"-\"))\n",
    "                motion_detected = False\n",
    "                b_left = int(annotation.frames[0].normalized_bounding_box.left * width)\n",
    "                b_right = int(annotation.frames[0].normalized_bounding_box.right * width)\n",
    "                b_top = int(annotation.frames[0].normalized_bounding_box.top * height)\n",
    "                b_bottom = int(annotation.frames[0].normalized_bounding_box.bottom * height)\n",
    "\n",
    "                f_index = 0\n",
    "                for frames in annotation.frames: \n",
    "                    box = frames.normalized_bounding_box\n",
    "                    f_left = int(box.left * width)\n",
    "                    f_right = int(box.right * width)\n",
    "                    f_top = int(box.top * height)\n",
    "                    f_bottom = int(box.bottom * height)\n",
    "                    \n",
    "                    if (abs(f_left - b_left) > 10 or abs(f_right - b_right) > 10 or abs(f_top - b_top ) > 10 or abs(f_bottom - b_bottom) > 10):\n",
    "                    #if (abs(f_left - b_left) > 5 or abs(f_right - b_right) > 5 or abs(f_top - b_top ) > 5 or abs(f_bottom - b_bottom) > 5):            \n",
    "                        motion_detected = True\n",
    "                        print(f\" > frame [{f_index}] : Object is moved\")\n",
    "                        print(\"    base box\", f\"({b_left}, {b_top})\", f\"({b_right}, {b_bottom})\",sep=\" | \",)\n",
    "                        print(\"    frame box\", f\"({f_left}, {f_top})\",f\"({f_right}, {f_bottom})\", sep=\" | \",)\n",
    "                        break\n",
    "                    else :\n",
    "                        motion_detected = False\n",
    "                    f_index +=1 \n",
    "                \n",
    "            if motion_detected :\n",
    "                break\n",
    "            a_index +=1 \n",
    "                    \n",
    "    if not motion_detected:\n",
    "        print(\"This video has just one shot and no motion\")\n",
    "    else:\n",
    "        print(\"This video has more than one shots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_detected_shot_change_extract_frame(results: vi.VideoAnnotationResults, video_name):\n",
    "    #print(results)\n",
    "    annotations = results.shot_annotations\n",
    "    #print(annotations)\n",
    "    \n",
    "    index=0;\n",
    "    images = [] \n",
    "\n",
    "    print(f\" Detected shots: {len(annotations)} \".center(80, \"-\"))\n",
    "    for annotation in annotations:\n",
    "        t1 = annotation.start_time_offset.total_seconds()\n",
    "        t2 = annotation.end_time_offset.total_seconds()\n",
    "\n",
    "        image = extract_shot_frames(index, video_name, t1, t2)\n",
    "        if image:\n",
    "            images.append(image)\n",
    "            index += 1\n",
    "\n",
    "            print_image_frames(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_uri = video_gcs_uri\n",
    "video_name = video_file\n",
    "results = detect_shot_change(video_uri )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shot 변경 여부 감지 및 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_detected_shot_change(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# shot 변경 감지 및 shot의 Frame 이미지 extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_detected_shot_change_extract_frame(results, video_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이미지 품질 검사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade scikit-image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.metrics import mean_squared_error\n",
    "#from skimage.feature import brisque\n",
    "\n",
    "def is_low_quality(image_path, blur_threshold=100, noise_threshold=0.005, contrast_threshold=0.9):\n",
    "    img = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    low_quality = False\n",
    "\n",
    "    # Blur detection\n",
    "    laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "    print(f\"laplacian_var : {laplacian_var}\")\n",
    "    if laplacian_var < blur_threshold:\n",
    "        print(\"low blur\")\n",
    "        quality = True\n",
    "\n",
    "    # Noise detection\n",
    "    mse = mean_squared_error(gray, np.full_like(gray, 128))  # Compare to flat gray\n",
    "    print(f\"mse : {mse}\")\n",
    "    if mse > noise_threshold:\n",
    "        print(\"noise\")\n",
    "        quality = True\n",
    "   \n",
    "    # Contrast detection\n",
    "    hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n",
    "    cdf = hist.cumsum() / hist.sum()\n",
    "    contrast_ratio = (cdf[255] - cdf[0]) / cdf[255]\n",
    "    print(f\"contrast_ratio : {contrast_ratio}\")\n",
    "    if contrast_ratio < contrast_threshold:\n",
    "        print(\"contrast\")\n",
    "        quality = True\n",
    "   \n",
    "    # Optional: BRISQUE score for a more sophisticated quality estimate\n",
    "    \"\"\"\n",
    "    brisque_score = brisque.score(gray)\n",
    "    if brisque_score > 50:  # Experimentally tune this threshold\n",
    "         return True\n",
    "    \"\"\"\n",
    "    return quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "image_path = './logos/video1.mp4/0_frame.jpg'\n",
    "if is_low_quality(image_path):\n",
    "    print(\"Low quality image detected\")\n",
    "else:\n",
    "    print(\"Image quality acceptable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "image_path = './logos/video1.mp4/0_frame.jpg'\n",
    "if is_low_quality(image_path):\n",
    "    print(\"Low quality image detected\")\n",
    "else:\n",
    "    print(\"Image quality acceptable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def edge_quality_check(image_path, threshold=50):\n",
    "    img = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    edges = cv2.Canny(gray, 50, 300) \n",
    "    percent_strong_edges = np.sum(edges > 128) / edges.size\n",
    "    print(f\"{edges} { edges.size} {percent_strong_edges}\")\n",
    "\n",
    "    if percent_strong_edges < threshold:\n",
    "        return True  # Indicates low quality\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_path = './logos/video1.mp4/0_frame.jpg'\n",
    "edge_quality_check(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image 속성, 조작 관련 함수들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RGB 값 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def extract_rgb_pillow(image_path):\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # Convert to RGB mode if needed\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "\n",
    "    width, height = img.size\n",
    "    rgb_pixels = []\n",
    "\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            r, g, b = img.getpixel((x, y))\n",
    "            rgb_pixels.append((r, g, b))\n",
    "\n",
    "    return rgb_pixels\n",
    "\n",
    "# Example usage\n",
    "image_path = './logos/video1.mp4/0_frame.jpg'\n",
    "rgb_data = extract_rgb_pillow(image_path)\n",
    "print(rgb_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그림 위에 격자 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def draw_grid(image_name, grid_shape, color='red', thickness=1):\n",
    "    img = Image.open(image_name)  \n",
    "    rows, cols = grid_shape\n",
    "    width, height = img.size\n",
    "\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    # Draw vertical lines\n",
    "    for i in range(1, cols): \n",
    "        x = width * i / cols\n",
    "        draw.line((x, 0, x, height), fill=color, width=thickness)\n",
    "\n",
    "    # Draw horizontal lines\n",
    "    for i in range(1, rows):\n",
    "        y = height * i / rows\n",
    "        draw.line((0, y, width, y), fill=color, width=thickness)\n",
    "\n",
    "    return img\n",
    "\n",
    "# Example usage\n",
    "image_path = './logos/video1.mp4/logo_0.jpg'\n",
    "grid_img = draw_grid(image_path, (10, 20))  # Draw red grid lines\n",
    "grid_img.save(\"./logo_grid.jpg\")\n",
    "\n",
    "display(grid_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB 값 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def get_grid_average_rgb(image_path, grid_shape, grid_position):\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    rows, cols = grid_shape\n",
    "    row_index, col_index = grid_position\n",
    "\n",
    "    # Calculate cell size\n",
    "    cell_width, cell_height = img.size[0] // cols, img.size[1] // rows\n",
    "\n",
    "    # Select the grid cell\n",
    "    start_x = col_index * cell_width\n",
    "    start_y = row_index * cell_height\n",
    "    end_x = start_x + cell_width\n",
    "    end_y = start_y + cell_height\n",
    "    grid_cell_img = img.crop((start_x, start_y, end_x, end_y))\n",
    "\n",
    "    # Convert to a NumPy array for efficient calculations\n",
    "    grid_cell_array = np.array(grid_cell_img)\n",
    "\n",
    "    # Calculate average RGB\n",
    "    average_rgb = np.mean(grid_cell_array, axis=(0, 1)).astype(int)\n",
    "\n",
    "    #return average_rgb[0],  average_rgb[1], average_rgb[2]\n",
    "    return average_rgb\n",
    "\n",
    "def get_grid_rgb(image_path, grid_shape, grid_position, select_corner=\"top_left\"):\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    rows, cols = grid_shape\n",
    "    row_index, col_index = grid_position\n",
    "\n",
    "    cell_width, cell_height = img.size[0] // cols, img.size[1] // rows\n",
    "    start_x = col_index * cell_width\n",
    "    start_y = row_index * cell_height\n",
    "\n",
    "    if select_corner == \"top_left\":\n",
    "        first_pixel = img.getpixel((start_x, start_y))\n",
    "    elif select_corner == \"center\":\n",
    "        center_x = start_x + cell_width // 2\n",
    "        center_y = start_y + cell_height // 2\n",
    "        first_pixel = img.getpixel((center_x, center_y))\n",
    "    else:\n",
    "        raise ValueError(\"Invalid select_corner value. Use 'top_left' or 'center'\")\n",
    "\n",
    "    return first_pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_path = './logos/video1.mp4/logo_0.jpg'\n",
    "grid_shape = (10, 20)\n",
    "grid_position = (4, 6)\n",
    "\n",
    "rgb_color = get_grid_rgb(image_path, grid_shape, grid_position, select_corner=\"center\")\n",
    "print(f\"RGB color of the center pixel in grid cell (4, 6): {rgb_color}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RGB 색상 확인 용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "width = 20 \n",
    "height = 20\n",
    "color = (165, 0, 52)  # RGB color tuple\n",
    "\n",
    "# Create a new RGB image \n",
    "img = Image.new('RGB', (width, height), color)\n",
    "\n",
    "# Show the image\n",
    "display(img)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "rows = 20\n",
    "cols = 40\n",
    "data = np.zeros((rows, cols), dtype='object')\n",
    "\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        grid_position = (i, j)  \n",
    "        data[i,j] = get_grid_rgb(image_path, (rows,cols), grid_position, select_corner=\"center\")\n",
    "        \n",
    "\n",
    "# Parameters \n",
    "image_size = (40, 30) \n",
    "\n",
    "# Create canvas for the whole arrangement\n",
    "canvas_width = cols * image_size[0]\n",
    "canvas_height = rows * image_size[1]\n",
    "canvas = Image.new('RGB', (canvas_width, canvas_height), 'white')  # White background\n",
    "\n",
    "# Font for position text\n",
    "font = ImageFont.load_default()  \n",
    "\n",
    "# 2. Display images and positions\n",
    "for row in range(rows):\n",
    "    for col in range(cols):\n",
    "        text= \"\"\n",
    "        x = col * image_size[0]\n",
    "        y = row * image_size[1]       \n",
    "\n",
    "        color = tuple(data[row, col])\n",
    "\n",
    "        img = Image.new('RGB', image_size, color)\n",
    "\n",
    "        canvas.paste(img, (x, y))  # Paste image \n",
    "\n",
    "        # Draw position text\n",
    "        draw = ImageDraw.Draw(canvas)\n",
    "        text_position = (x + 2, y + 2)  # Small offset\n",
    "        if (color[0] == 165 and color[1] == 0 and color[2] == 52) :\n",
    "            print(\"color match\")\n",
    "            text = \"LR\" \n",
    "        draw.text(text_position, f\"{text}\", font=font, fill='black') \n",
    "\n",
    "canvas.save(\"logo_1.jpg\")\n",
    "display(canvas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def get_matching_pixels(image_path, target_rgb):\n",
    "    img = Image.open(image_path)\n",
    "    width, height = img.size\n",
    "    pixels = img.load()  # Get pixel access object\n",
    "\n",
    "    matching_pixels = []\n",
    "\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            current_rgb = pixels[x, y]\n",
    "            if current_rgb == target_rgb:\n",
    "                matching_pixels.append((x, y))\n",
    "\n",
    "    return matching_pixels\n",
    "\n",
    "# Example usage\n",
    "image_path = './logos/video1.mp4/logo_0.jpg'\n",
    "target_rgb = (165, 0, 52)  # Replace with your desired RGB value\n",
    "\n",
    "matching_pixels = get_matching_pixels(image_path, target_rgb)\n",
    "\n",
    "# Print pixel information\n",
    "for pixel in matching_pixels:\n",
    "    x, y = pixel\n",
    "    print(f\"Pixel at ({x}, {y}) has the RGB value {target_rgb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "def is_audio_silent_librosa(video_file, threshold=0.02):\n",
    "    \"\"\"Detects silence using librosa for more fine-grained analysis.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(video_file)  # Load audio\n",
    "        rms = librosa.feature.rms(y=y)   # Calculate Root-Mean-Square (RMS) energy\n",
    "\n",
    "        if (rms < threshold).all():\n",
    "            #print(\"The video file has a SILENT audio stream.\")\n",
    "            return True  # Audio is mostly silent\n",
    "        else:\n",
    "            #print(\"The audio stream contains SOUND.\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file '{video_file}': {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "is_audio_silent_librosa(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision API로 Image에서 Logo Detect & 이미지 자르기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = vision.ImageAnnotatorClient()\n",
    "    \n",
    "image_file = \"./logos/video1.mp4/0_frame.jpg\"\n",
    "image = vision.Image(content=content)\n",
    "im = Image.open(image_file)    \n",
    "\n",
    "response = client.logo_detection(image=image)\n",
    "logos = response.logo_annotations\n",
    "print(\"Logos:\")\n",
    "\n",
    "for logo in logos:\n",
    "    print(logo)\n",
    "    vects = logo.bounding_poly.vertices\n",
    "        \n",
    "    im2 = im.crop([vects[0].x, vects[0].y , vects[2].x , vects[2].y ])\n",
    "    im2.save(\"vision-crop2.jpg\", \"JPEG\")\n",
    "    \n",
    "    im3 = Image.open(image_file)    \n",
    "    draw = ImageDraw.Draw(im3)\n",
    "    draw.polygon(\n",
    "        [\n",
    "            vects[0].x,\n",
    "            vects[0].y,\n",
    "            vects[1].x,\n",
    "            vects[1].y,\n",
    "            vects[2].x,\n",
    "            vects[2].y,\n",
    "            vects[3].x,\n",
    "            vects[3].y,\n",
    "        ],\n",
    "        None,\n",
    "        \"red\",\n",
    "    )\n",
    "    im3.save(\"output-hint.jpg\", \"JPEG\")\n",
    "\n",
    "if response.error.message:\n",
    "    raise Exception(\n",
    "        \"{}\\nFor more info on error messages, check: \"\n",
    "        \"https://cloud.google.com/apis/design/errors\".format(response.error.message)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision API로 Logo Detect하는 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_logo_by_vision(image_name):\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    with open(image_name, \"rb\") as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = vision.Image(content=content)\n",
    "\n",
    "    response = client.logo_detection(image=image)\n",
    "    logos = response.logo_annotations\n",
    "   \n",
    "    logo_names = []\n",
    "    logo_count = 0\n",
    "\n",
    "    if not os.path.exists(image_name):\n",
    "        os.makedirs(image_name) \n",
    "    \n",
    "    for logo in logos:\n",
    "        print(logo.description)\n",
    "        vects = logo.bounding_poly.vertices\n",
    "        \n",
    "        im2 = im.crop([vects[0].x, vects[0].y , vects[2].x , vects[2].y ])\n",
    "        logo_name = f\"logo_{logo_count}\"\n",
    "        im2.save(logo_name, \"JPEG\")\n",
    "        logo_names.append(logo_name)\n",
    "        logo_count += 1\n",
    "\n",
    "    if response.error.message:\n",
    "        raise Exception(\n",
    "            \"{}\\nFor more info on error messages, check: \"\n",
    "            \"https://cloud.google.com/apis/design/errors\".format(response.error.message)\n",
    "        )\n",
    "\n",
    "    print_image_frames(logo_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_file = \"./logos/video1.mp4/0_frame.jpg\"\n",
    "extract_logo_by_vision(image_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전체 프레임 수 및 해상도 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def get_video_info(video_path):\n",
    "    # Create a VideoCapture object to read the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Check if the video was opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        exit()\n",
    "\n",
    "    # Get the total number of frames\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Get the frame rate\n",
    "    frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Get the width and height of the video\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Print the extracted information\n",
    "    print(\"* Total frames:\", total_frames)\n",
    "    print(\"* Frame rate:\", frame_rate)\n",
    "    print(\"* Resolution:\", width, \"x\", height)\n",
    "\n",
    "    # Release the VideoCapture object\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "im3 = Image.open(\"./logos/video1.mp4/0_frame.jpg\") \n",
    "print(im3.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9:16 사이즈 오용 검사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def is_shorts_format(video_path):\n",
    "    #print(video_path)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    ret, first_frame = cap.read()\n",
    "    if ret:\n",
    "        cv2.imwrite(\"./shorts_temp.jpg\", first_frame)\n",
    "        #print(f\"image name : {image_name}\")\n",
    "    else :\n",
    "        print(\"error to read video\")\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Load the image\n",
    "    is_shorts = False\n",
    "    \n",
    "    image = cv2.imread(\"./shorts_temp.jpg\")    \n",
    "\n",
    "\n",
    "    # Convert to grayscale for easier processing\n",
    "    grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    height, width = grayscale_image.shape[:2]\n",
    "\n",
    "    # Check for all-black pixels in both regions\n",
    "    left_region = grayscale_image[:, 0:400]  \n",
    "    right_region_start = width - 400  \n",
    "    right_region = grayscale_image[:, right_region_start:width]  \n",
    "\n",
    "    left_is_all_black = all(pixel == 0 for pixel in left_region.flatten())\n",
    "    right_is_all_black = all(pixel == 0 for pixel in right_region.flatten())\n",
    "\n",
    "    if left_is_all_black and right_is_all_black :\n",
    "        print(\"This is a shorts format video\")\n",
    "        is_shorts = True\n",
    "\n",
    "    # Print the results\n",
    "    #print(\"The left region (0-400 pixels) is all black:\", left_is_all_black)\n",
    "    #print(\"The right region (0-400 pixels from the right) is all black:\", right_is_all_black)\n",
    "    \n",
    "    return is_shorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "is_shorts = is_shorts_format(video_path)\n",
    "get_video_info(video_path)\n",
    "print(f\"* Is this shorts format : {is_shorts}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Sports AI Analysis",
   "provenance": []
  },
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
